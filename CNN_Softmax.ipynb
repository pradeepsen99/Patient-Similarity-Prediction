{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4mPcC1aa83O"
      },
      "source": [
        "# CNN_Softmax - CS 598 DLH - Reproducable Paper Final Project\n",
        "\n",
        "This contains the code required for the paper titled \"A Novel Deep Similarity Learning Approach to Electronic Health Records Data\"\n",
        "\n",
        "Please note some of the code portions might have been used from MY OWN code of CS598-DLH lab/mps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PTR-kLB4FG0q"
      },
      "source": [
        "## Environment setup + Data pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v1fal44MBuSf",
        "outputId": "0ed82460-9f5a-4a48-f711-13335789f5dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.7/dist-packages (4.4.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from gdown) (4.64.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from gdown) (3.6.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.7/dist-packages (from gdown) (2.23.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from gdown) (4.6.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gdown) (1.15.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (1.24.3)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "mkdir: cannot create directory ‘data’: File exists\n",
            "NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Environment setup\n",
        "!pip3 install gdown\n",
        "!mkdir data\n",
        "!nvidia-smi\n",
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "\n",
        "#Imports\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import gdown\n",
        "import pandas as pd\n",
        "from tabulate import tabulate\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn as nn\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PlNpKdN1oHJ7",
        "outputId": "4de31c4d-4440-4d0a-9dd0-46a8c0c22168"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1uBv9j602LGyN43wvbQDpvka49rR9eNUQ\n",
            "To: /content/data/orbda.csv\n",
            "100%|██████████| 818M/818M [00:06<00:00, 130MB/s]\n"
          ]
        }
      ],
      "source": [
        "#https://drive.google.com/uc?id=1uBv9j602LGyN43wvbQDpvka49rR9eNUQ&export=download&confirm=t\n",
        "'''from google_drive_downloader import GoogleDriveDownloader as gdd\n",
        "gdd.download_file_from_google_drive(file_id='1uBv9j602LGyN43wvbQDpvka49rR9eNUQ',\n",
        "                                    dest_path='./data/orbda.csv',\n",
        "                                    unzip=True,\n",
        "                                    showsize=True,\n",
        "                                    overwrite=True)'''\n",
        "\n",
        "\n",
        "url = \"https://drive.google.com/uc?id=1uBv9j602LGyN43wvbQDpvka49rR9eNUQ\"\n",
        "output = \"./data/orbda.csv\"\n",
        "gdown.download(url, output, quiet=False)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "df = pd.read_csv('data/orbda.csv', low_memory=False) #read in csv file (~800mb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f5hQPH09ET1m"
      },
      "outputs": [],
      "source": [
        "#Sanity checks for pandas import + read_csv\n",
        "#print(df.columns)\n",
        "#print(sorted(df['ap_cidpri'].unique()))\n",
        "#df.head()\n",
        "#print(df['id_'].nunique())\n",
        "#print(df['id_'].count())\n",
        "#print(df['ap_coduni'].unique())\n",
        "#print(df['ap_coduni'].count())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXqUq-5kAVw1"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4T5yj_6O1XAe"
      },
      "outputs": [],
      "source": [
        "#Filtering out for specific values as outlined in Table 3\n",
        "kidney_codes = ['E10 ', 'E14 ', 'I10 ', 'I120', 'N039', 'N088', 'N083', 'N180', 'N188', 'N189']\n",
        "df = df.loc[df['ap_cidpri'].isin(kidney_codes)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oPvj4uyPKBNN",
        "outputId": "1aafa818-e291-426d-ba19-e79c1db3d038"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----------+----------+------------+--------------+------------------+-------------+-------------+----------+----------+-------------+-------------+\n",
            "|    | an_hcv   | an_hiv   | an_hbsag   |   ap_nuidade | ap_coduni        |   ap_pripal |   ap_motsai | estado   |   an_tru |   an_intfis | ap_cidpri   |\n",
            "|----+----------+----------+------------+--------------+------------------+-------------+-------------+----------+----------+-------------+-------------|\n",
            "|  0 | N        | N        | N          |           25 | a1042cb8e9265d4e |   305010107 |          21 | MG       |     628  |          03 | N180        |\n",
            "|  1 | N        | N        | N          |           47 | 69ba059ff91532d3 |   305010107 |          21 | RJ       |     0065 |          00 | N180        |\n",
            "|  2 | N        | N        | N          |           15 | a2b516fa1aa3cce0 |   305010107 |          21 | PR       |     0    |          01 | N180        |\n",
            "|  3 | N        | N        | N          |           37 | 72f15d07e504318f |   305010107 |          21 | RJ       |     6582 |          01 | N180        |\n",
            "|  4 | N        | N        | N          |           71 | 72f15d07e504318f |   305010107 |          21 | RJ       |     7778 |          01 | N180        |\n",
            "+----+----------+----------+------------+--------------+------------------+-------------+-------------+----------+----------+-------------+-------------+\n"
          ]
        }
      ],
      "source": [
        "#Filtering out columns to match input features\n",
        "#Features: an_hcv, an_hiv, an_hbsag, ap_nuidade, ap_coduni, owner_id, ap_pripal, ap_motsai, estado, an_tru, an_intfis, vol\n",
        "column_features = ['an_hcv', 'an_hiv', 'an_hbsag', 'ap_nuidade', 'ap_coduni', 'ap_pripal', 'ap_motsai', 'estado', 'an_tru', 'an_intfis', 'ap_cidpri']\n",
        "df_input_features = df.filter(items=column_features)\n",
        "print(tabulate(df_input_features.head(), headers = 'keys', tablefmt = 'psql'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e3grtwvAA4CG",
        "outputId": "85fde27d-beca-4de3-f72f-6b879395fa45"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----------+----------+------------+--------------+-------------+-------------+-------------+----------+----------+-------------+-------------+\n",
            "|    | an_hcv   | an_hiv   | an_hbsag   |   ap_nuidade |   ap_coduni |   ap_pripal |   ap_motsai | estado   |   an_tru |   an_intfis | ap_cidpri   |\n",
            "|----+----------+----------+------------+--------------+-------------+-------------+-------------+----------+----------+-------------+-------------|\n",
            "|  0 | N        | N        | N          |           25 |           0 |   305010107 |          21 | MG       |     628  |          03 | N180        |\n",
            "|  1 | N        | N        | N          |           47 |           1 |   305010107 |          21 | RJ       |     0065 |          00 | N180        |\n",
            "|  2 | N        | N        | N          |           15 |           2 |   305010107 |          21 | PR       |     0    |          01 | N180        |\n",
            "|  3 | N        | N        | N          |           37 |           3 |   305010107 |          21 | RJ       |     6582 |          01 | N180        |\n",
            "|  4 | N        | N        | N          |           71 |           3 |   305010107 |          21 | RJ       |     7778 |          01 | N180        |\n",
            "+----+----------+----------+------------+--------------+-------------+-------------+-------------+----------+----------+-------------+-------------+\n"
          ]
        }
      ],
      "source": [
        "#Converting ap_coduni values into int via dict\n",
        "coduni_dict = {}\n",
        "unique_coduni = df_input_features['ap_coduni'].unique()\n",
        "for i in range(0, len(unique_coduni)):\n",
        "  coduni_dict[unique_coduni[i]] = i\n",
        "df_input_features = df_input_features.replace({'ap_coduni': coduni_dict})\n",
        "print(tabulate(df_input_features.head(), headers = 'keys', tablefmt = 'psql'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WnlKRngRCaka",
        "outputId": "30b36a99-f795-411e-c4cb-cd3e2880fff5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----------+----------+------------+--------------+-------------+-------------+-------------+----------+----------+-------------+-------------+\n",
            "|    |   an_hcv |   an_hiv |   an_hbsag |   ap_nuidade |   ap_coduni |   ap_pripal |   ap_motsai | estado   |   an_tru |   an_intfis | ap_cidpri   |\n",
            "|----+----------+----------+------------+--------------+-------------+-------------+-------------+----------+----------+-------------+-------------|\n",
            "|  0 |        0 |        0 |          0 |           25 |           0 |   305010107 |          21 | MG       |     628  |          03 | N180        |\n",
            "|  1 |        0 |        0 |          0 |           47 |           1 |   305010107 |          21 | RJ       |     0065 |          00 | N180        |\n",
            "|  2 |        0 |        0 |          0 |           15 |           2 |   305010107 |          21 | PR       |     0    |          01 | N180        |\n",
            "|  3 |        0 |        0 |          0 |           37 |           3 |   305010107 |          21 | RJ       |     6582 |          01 | N180        |\n",
            "|  4 |        0 |        0 |          0 |           71 |           3 |   305010107 |          21 | RJ       |     7778 |          01 | N180        |\n",
            "+----+----------+----------+------------+--------------+-------------+-------------+-------------+----------+----------+-------------+-------------+\n"
          ]
        }
      ],
      "source": [
        "#Converting an_hcv, an_hiv, an_hbsag to int values from dict\n",
        "yes_no_dict = {'N': 0, \"P\": 1}\n",
        "df_input_features = df_input_features.replace({'an_hcv': yes_no_dict, 'an_hiv': yes_no_dict, 'an_hbsag': yes_no_dict})\n",
        "print(tabulate(df_input_features.head(), headers = 'keys', tablefmt = 'psql'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1zZ0wVXJG71",
        "outputId": "260624b2-b35b-4b6b-ade4-a79407dc77c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----------+----------+------------+--------------+-------------+-------------+-------------+----------+----------+-------------+-------------+\n",
            "|    |   an_hcv |   an_hiv |   an_hbsag |   ap_nuidade |   ap_coduni |   ap_pripal |   ap_motsai |   estado |   an_tru |   an_intfis | ap_cidpri   |\n",
            "|----+----------+----------+------------+--------------+-------------+-------------+-------------+----------+----------+-------------+-------------|\n",
            "|  0 |        0 |        0 |          0 |           25 |           0 |   305010107 |          21 |        0 |     628  |          03 | N180        |\n",
            "|  1 |        0 |        0 |          0 |           47 |           1 |   305010107 |          21 |        1 |     0065 |          00 | N180        |\n",
            "|  2 |        0 |        0 |          0 |           15 |           2 |   305010107 |          21 |        2 |     0    |          01 | N180        |\n",
            "|  3 |        0 |        0 |          0 |           37 |           3 |   305010107 |          21 |        1 |     6582 |          01 | N180        |\n",
            "|  4 |        0 |        0 |          0 |           71 |           3 |   305010107 |          21 |        1 |     7778 |          01 | N180        |\n",
            "+----+----------+----------+------------+--------------+-------------+-------------+-------------+----------+----------+-------------+-------------+\n"
          ]
        }
      ],
      "source": [
        "#Converting estado to int values from dict\n",
        "estado_dict = {}\n",
        "unique_estado = df_input_features['estado'].unique()\n",
        "for i in range(0, len(unique_estado)):\n",
        "  estado_dict[unique_estado[i]] = i\n",
        "df_input_features = df_input_features.replace({'estado': estado_dict})\n",
        "print(tabulate(df_input_features.head(), headers = 'keys', tablefmt = 'psql'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CBLWyefuPMhE"
      },
      "outputs": [],
      "source": [
        "#Converting an_tru to int values from dict \n",
        "df_input_features['an_tru'] = df_input_features['an_tru'].str.strip()\n",
        "df_input_features['an_tru'] = df_input_features['an_tru'].str.extract('(\\d+)', expand=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oOKgAGypMdL9",
        "outputId": "1809ed8a-8e25-4693-8252-c52a55de9ecb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----------+----------+------------+--------------+-------------+-------------+-------------+----------+----------+-------------+-------------+\n",
            "|    |   an_hcv |   an_hiv |   an_hbsag |   ap_nuidade |   ap_coduni |   ap_pripal |   ap_motsai |   estado |   an_tru |   an_intfis | ap_cidpri   |\n",
            "|----+----------+----------+------------+--------------+-------------+-------------+-------------+----------+----------+-------------+-------------|\n",
            "|  0 |        0 |        0 |          0 |           25 |           0 |   305010107 |          21 |        0 |      628 |           0 | N180        |\n",
            "|  1 |        0 |        0 |          0 |           47 |           1 |   305010107 |          21 |        1 |     0065 |           1 | N180        |\n",
            "|  2 |        0 |        0 |          0 |           15 |           2 |   305010107 |          21 |        2 |        0 |           2 | N180        |\n",
            "|  3 |        0 |        0 |          0 |           37 |           3 |   305010107 |          21 |        1 |     6582 |           2 | N180        |\n",
            "|  4 |        0 |        0 |          0 |           71 |           3 |   305010107 |          21 |        1 |     7778 |           2 | N180        |\n",
            "+----+----------+----------+------------+--------------+-------------+-------------+-------------+----------+----------+-------------+-------------+\n"
          ]
        }
      ],
      "source": [
        "#Converting an_intfis to int values from dict \n",
        "\n",
        "intfis_dict = {}\n",
        "unique_intfis = df_input_features['an_intfis'].unique()\n",
        "for i in range(0, len(unique_intfis)):\n",
        "  intfis_dict[unique_intfis[i]] = i\n",
        "df_input_features = df_input_features.replace({'an_intfis': intfis_dict})\n",
        "print(tabulate(df_input_features.head(), headers = 'keys', tablefmt = 'psql'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZStaoWUyLjB4"
      },
      "outputs": [],
      "source": [
        "#Converting pandas object types into integer\n",
        "df_input_features = df_input_features.dropna()\n",
        "df_input_features['an_hcv'] = df_input_features['an_hcv'].astype('str').astype('float')\n",
        "df_input_features['an_hiv'] = df_input_features['an_hiv'].astype('str').astype('float')\n",
        "df_input_features['an_hbsag'] = df_input_features['an_hbsag'].astype('str').astype('float')\n",
        "df_input_features['an_tru'] = df_input_features['an_tru'].astype('str').astype('float')\n",
        "df_input_features['an_intfis'] = df_input_features['an_intfis'].astype('str').astype('float')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h89LWhmqB5sk",
        "outputId": "23db6d33-9196-4f6b-850c-1a59d412027c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----------------+\n",
            "|      |       ap_cidpri |\n",
            "|------+-----------------|\n",
            "| N180 |     3.94376e+06 |\n",
            "| N189 | 38160           |\n",
            "| I120 | 19460           |\n",
            "| N039 |  7349           |\n",
            "| I10  |  5979           |\n",
            "| N083 |  5782           |\n",
            "| N188 |  4670           |\n",
            "| N088 |  1698           |\n",
            "| E10  |   190           |\n",
            "| E14  |   168           |\n",
            "+------+-----------------+\n",
            "+----+----------+----------+------------+--------------+-------------+-------------+-------------+----------+----------+-------------+-------------+\n",
            "|    |   an_hcv |   an_hiv |   an_hbsag |   ap_nuidade |   ap_coduni |   ap_pripal |   ap_motsai |   estado |   an_tru |   an_intfis | ap_cidpri   |\n",
            "|----+----------+----------+------------+--------------+-------------+-------------+-------------+----------+----------+-------------+-------------|\n",
            "|  0 |        0 |        0 |          0 |           25 |           0 |   305010107 |          21 |        0 |      628 |           0 | N180        |\n",
            "|  1 |        0 |        0 |          0 |           47 |           1 |   305010107 |          21 |        1 |       65 |           1 | N180        |\n",
            "|  2 |        0 |        0 |          0 |           15 |           2 |   305010107 |          21 |        2 |        0 |           2 | N180        |\n",
            "|  3 |        0 |        0 |          0 |           37 |           3 |   305010107 |          21 |        1 |     6582 |           2 | N180        |\n",
            "|  4 |        0 |        0 |          0 |           71 |           3 |   305010107 |          21 |        1 |     7778 |           2 | N180        |\n",
            "+----+----------+----------+------------+--------------+-------------+-------------+-------------+----------+----------+-------------+-------------+\n",
            "an_hcv        float64\n",
            "an_hiv        float64\n",
            "an_hbsag      float64\n",
            "ap_nuidade      int64\n",
            "ap_coduni       int64\n",
            "ap_pripal       int64\n",
            "ap_motsai       int64\n",
            "estado          int64\n",
            "an_tru        float64\n",
            "an_intfis     float64\n",
            "ap_cidpri      object\n",
            "dtype: object\n"
          ]
        }
      ],
      "source": [
        "#Sanity checks post filtering\n",
        "#display(df.groupby('ap_cidpri')['ap_cidpri'].transform('count'))\n",
        "print(tabulate(df['ap_cidpri'].value_counts().to_frame(), headers = 'keys', tablefmt = 'psql'))\n",
        "print(tabulate(df_input_features.head(), headers = 'keys', tablefmt = 'psql'))\n",
        "print(df_input_features.dtypes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vvesj7AoFPhC"
      },
      "outputs": [],
      "source": [
        "#Converting categorical lables to one hot encoding\n",
        "kidney_codes_dict = {'E10 ': 1, 'E14 ': 2, 'I10 ': 3, 'I120': 4, 'N039': 4, 'N088': 5, 'N083': 6, 'N180': 7, 'N188': 8, 'N189': 9}\n",
        "\n",
        "cidpri_list = df_input_features['ap_cidpri'].tolist()\n",
        "for i in range(0, len(cidpri_list)):\n",
        "  cidpri_list[i] = kidney_codes_dict[cidpri_list[i]]\n",
        "\n",
        "tensor_cidpri = torch.tensor(cidpri_list)\n",
        "cidpri_one_hot = F.one_hot(tensor_cidpri)\n",
        "\n",
        "#Drop cidpri column since we are now done with it\n",
        "df_input_features = df_input_features.drop(['ap_cidpri'], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2vRPoBS8qlzw"
      },
      "outputs": [],
      "source": [
        "#Dataloader calss\n",
        "import random\n",
        "class NephrologyDataset(Dataset):\n",
        "  def __init__(self, input_features, categorical_features):\n",
        "    x = input_features.values\n",
        "    y = categorical_features\n",
        "    #print(y)\n",
        "\n",
        "    self.x_train = x#torch.tensor(x, device=device)\n",
        "    self.y_train = cidpri_list#y#y.to(device)#torch.tensor(y, device=device)\n",
        "\n",
        "  def __len__(self):\n",
        "    return math.floor(len(self.y_train)/2)\n",
        "  \n",
        "  def __getitem__(self, idx):\n",
        "    if (idx + 2> self.__len__()):\n",
        "      idx = idx-3\n",
        "    second_idx = random.randint(0, self.__len__()-1)\n",
        "    return torch.tensor(self.x_train[idx], device=device), torch.tensor(self.x_train[second_idx], device=device), torch.tensor((self.y_train[second_idx] == self.y_train[idx]), device=device).float()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7YTJRDfGsZIN"
      },
      "outputs": [],
      "source": [
        "#Dataloader initialization\n",
        "split_train = int((len(cidpri_one_hot)) * .8)\n",
        "split_test = int((len(cidpri_one_hot)) * .2)\n",
        "dataloader_train = NephrologyDataset(df_input_features[:split_train], cidpri_list[:split_train])\n",
        "dataloader_test = NephrologyDataset(df_input_features[:-split_test], cidpri_list[:-split_test])\n",
        "train_loader = DataLoader(dataloader_train,batch_size=10000,shuffle=True, drop_last=True)\n",
        "test_loader = DataLoader(dataloader_test,batch_size=10000,shuffle=True, drop_last=True)\n",
        "\n",
        "sample_ex = next(iter(test_loader))\n",
        "print(len(sample_ex))\n",
        "x, y, l = sample_ex\n",
        "print(x[0])\n",
        "print(y[0])\n",
        "print(l[0])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#CNN_Softmax model definition\n",
        "class CNN_Eucledian(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv_a = torch.nn.Conv1d(in_channels=10, \n",
        "                                      out_channels=25, \n",
        "                                      kernel_size=1, bias=False)\n",
        "        self.conv_b = torch.nn.Conv1d(in_channels=25, \n",
        "                                      out_channels=37, \n",
        "                                      kernel_size=1)\n",
        "        self.maxpool_a = torch.nn.MaxPool1d(kernel_size=1,\n",
        "                                            stride=1)\n",
        "        self.maxpool_b = torch.nn.MaxPool1d(kernel_size=1,\n",
        "                                            stride=1)\n",
        "        self.fc = torch.nn.Linear(in_features=37, \n",
        "                                  out_features=10)\n",
        "        self.sm_fc = torch.nn.Linear(in_features=10000,\n",
        "                                     out_features=1)\n",
        "        self.sm_fc2 = torch.nn.Linear(in_features=10,\n",
        "                                      out_features=1)\n",
        "        self.sm = torch.nn.Softmax(dim=1)\n",
        "\n",
        "    def forward_single(self, x):\n",
        "        #x = torch.permute(x, (0, 2, 1))\n",
        "        x = self.conv_a(x)\n",
        "        x = F.relu(x, inplace=True)\n",
        "        x = self.maxpool_a(x)\n",
        "\n",
        "        #print('Finished first conv')\n",
        "        x = self.conv_b(x)\n",
        "        x = F.relu(x, inplace=True)\n",
        "        x = self.maxpool_b(x).squeeze()\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        #for name, param in self.named_parameters():\n",
        "        x = x.unsqueeze(dim=2).float()\n",
        "        y = y.unsqueeze(dim=2).float()\n",
        "        patient1 = self.forward_single(x)\n",
        "        patient2 = self.forward_single(y)\n",
        "\n",
        "        dist = torch.cdist(patient1, patient2)\n",
        "        dist = self.sm(dist)\n",
        "        dist = self.sm_fc(dist)\n",
        "        print(dist.shape)\n",
        "        return 1-torch.abs(dist)\n"
      ],
      "metadata": {
        "id": "1VtNTkSk0lRI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nFtW3v_KVuuR"
      },
      "outputs": [],
      "source": [
        "#Test/Validate model\n",
        "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def validate(loader, model, criterion):                       \n",
        "    correct = 0                                               \n",
        "    total = 0                                                 \n",
        "    running_loss = []\n",
        "    model.eval()                                              \n",
        "    with torch.no_grad():                                     \n",
        "        for i, data in enumerate((loader)):\n",
        "            x, y, label = data      \n",
        "                                                              \n",
        "            outputs = net(x, y)\n",
        "            outputs = outputs > 0.5\n",
        "            outputs = outputs.float()\n",
        "            loss = criterion(outputs, label.unsqueeze(dim=1)) \n",
        "            #print(outputs.shape)\n",
        "            outputs = torch.round(outputs)\n",
        "            total += label.shape[0]\n",
        "            correct += np.sum(outputs.squeeze().tolist() == label.squeeze().tolist())\n",
        "            running_loss.append(loss.item())\n",
        "            print(correct)      \n",
        "    mean_val_accuracy = (100 * correct / total)               \n",
        "    mean_val_loss = np.mean(running_loss)                  \n",
        "    mean_val_accuracy = accuracy(outputs,labels)       \n",
        "    print('Validation Accuracy: %d %%' % (mean_val_accuracy)) \n",
        "    print('Validation Loss:'  ,mean_val_loss )  \n",
        "\n",
        "def eval(model, val_loader):\n",
        "    \n",
        "    \"\"\"\n",
        "    Evaluate the model.\n",
        "    \n",
        "    Arguments:\n",
        "        model: the RNN model\n",
        "        val_loader: validation dataloader\n",
        "        \n",
        "    Outputs:\n",
        "        precision: overall precision score\n",
        "        recall: overall recall score\n",
        "        f1: overall f1 score\n",
        "        roc_auc: overall roc_auc score\n",
        "        \n",
        "    REFERENCE: checkout https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics\n",
        "    \"\"\"\n",
        "    \n",
        "    model.eval()\n",
        "    y_pred = torch.LongTensor()\n",
        "    y_score = torch.Tensor()\n",
        "    y_true = torch.LongTensor()\n",
        "    model.eval()\n",
        "    for i, data in enumerate(tqdm(val_loader)):\n",
        "        x, y, label = data      \n",
        "        outputs = net(x, y)\n",
        "\n",
        "        #https://discuss.pytorch.org/t/confused-about-binary-classification-with-pytorch/83759/10\n",
        "        y_hat = (outputs >= 0.8).float()\n",
        "        #print(y_hat[:10])\n",
        "        \n",
        "        y_pred = torch.cat((y_pred,  y_hat.detach().to('cpu')), dim=0)\n",
        "        y_true = torch.cat((y_true, label.detach().to('cpu')), dim=0)\n",
        "    \n",
        "    p, r, f, _ = precision_recall_fscore_support(y_true, y_pred, average='binary')\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    return p, r, f, acc\n",
        "\n",
        "p, r, f, acc = eval(net, test_loader)\n",
        "print(p)\n",
        "print(r)\n",
        "print(f)\n",
        "print(acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tsRVyI_vAmzS"
      },
      "outputs": [],
      "source": [
        "#CNN_Softmax model definition\n",
        "class CNN_Softmax_old(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv_a = torch.nn.Conv1d(in_channels=10, \n",
        "                                      out_channels=25, \n",
        "                                      kernel_size=1, bias=False)\n",
        "        self.conv_b = torch.nn.Conv1d(in_channels=25, \n",
        "                                      out_channels=37, \n",
        "                                      kernel_size=1)\n",
        "        self.maxpool_a = torch.nn.MaxPool1d(kernel_size=1,\n",
        "                                            stride=1)\n",
        "        self.maxpool_b = torch.nn.MaxPool1d(kernel_size=1,\n",
        "                                            stride=1)\n",
        "        self.fc = torch.nn.Linear(in_features=37, \n",
        "                                  out_features=10)\n",
        "        self.sm_fc = torch.nn.Linear(in_features=10010,\n",
        "                                     out_features=1)\n",
        "        self.sm_fc2 = torch.nn.Linear(in_features=10,\n",
        "                                      out_features=1)\n",
        "        self.sm = torch.nn.Softmax(dim=1)\n",
        "\n",
        "    def forward_single(self, x):\n",
        "        #x = torch.permute(x, (0, 2, 1))\n",
        "        x = self.conv_a(x)\n",
        "        x = F.relu(x, inplace=True)\n",
        "        x = self.maxpool_a(x)\n",
        "\n",
        "        #print('Finished first conv')\n",
        "        x = self.conv_b(x)\n",
        "        x = F.relu(x, inplace=True)\n",
        "        x = self.maxpool_b(x).squeeze()\n",
        "        x = self.fc(x)\n",
        "        #x = F.relu(x)\n",
        "        #x = self.sm(x)\n",
        "        #print(x.shape)\n",
        "        #x = self.sm_fc2(x)\n",
        "        #print(x.shape)\n",
        "        return x\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        #for name, param in self.named_parameters():\n",
        "          #print(name, param.shape)\n",
        "        x = x.unsqueeze(dim=2).float()\n",
        "        y = y.unsqueeze(dim=2).float()\n",
        "        patient1 = self.forward_single(x)\n",
        "        patient2 = self.forward_single(y)\n",
        "\n",
        "        matching_matrix = torch.matmul(self.fc.weight, self.fc.weight.T)\n",
        "\n",
        "        S = torch.matmul(patient1, patient2.T)\n",
        "        K = torch.add(patient1, patient2)\n",
        "\n",
        "        #print(\"S: \" + str(S.shape))\n",
        "        #print(\"K: \" + str(K.shape))\n",
        "        ks_cat = torch.cat((S, K), dim=1)\n",
        "        #print(\"kscat: \" + str(ks_cat.shape))\n",
        "        #print(score.shape)\n",
        "        score=ks_cat\n",
        "        score = self.sm(score)\n",
        "        #score = F.relu(x).squeeze()\n",
        "        #print(score.shape)\n",
        "        score = self.sm_fc(score)\n",
        "        #score = torch.round(score)\n",
        "        #score = (torch.max(K, dim=1)[0]).float()\n",
        "        #score = score.clone().detach().requires_grad_(True)\n",
        "        #print(\"score: \" + str(score.shape))\n",
        "        #print(score[:11])\n",
        "        return 1-torch.abs(score)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#CNN_Softmax model definition\n",
        "class CNN_Cosine(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv_a = torch.nn.Conv1d(in_channels=10, \n",
        "                                      out_channels=25, \n",
        "                                      kernel_size=1, bias=False)\n",
        "        self.conv_b = torch.nn.Conv1d(in_channels=25, \n",
        "                                      out_channels=37, \n",
        "                                      kernel_size=1)\n",
        "        self.maxpool_a = torch.nn.MaxPool1d(kernel_size=1,\n",
        "                                            stride=1)\n",
        "        self.maxpool_b = torch.nn.MaxPool1d(kernel_size=1,\n",
        "                                            stride=1)\n",
        "        self.fc = torch.nn.Linear(in_features=37, \n",
        "                                  out_features=10)\n",
        "        self.sm_fc = torch.nn.Linear(in_features=10000,\n",
        "                                     out_features=1)\n",
        "        self.sm_fc2 = torch.nn.Linear(in_features=10,\n",
        "                                      out_features=1)\n",
        "        self.cos = torch.nn.CosineSimilarity(dim=1)\n",
        "        self.sm = torch.nn.Softmax(dim=1)\n",
        "\n",
        "    def forward_single(self, x):\n",
        "        #x = torch.permute(x, (0, 2, 1))\n",
        "        x = self.conv_a(x)\n",
        "        x = F.relu(x, inplace=True)\n",
        "        x = self.maxpool_a(x)\n",
        "\n",
        "        #print('Finished first conv')\n",
        "        x = self.conv_b(x)\n",
        "        x = F.relu(x, inplace=True)\n",
        "        x = self.maxpool_b(x).squeeze()\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        #for name, param in self.named_parameters():\n",
        "        x = x.unsqueeze(dim=2).float()\n",
        "        y = y.unsqueeze(dim=2).float()\n",
        "        patient1 = self.forward_single(x)\n",
        "        patient2 = self.forward_single(y)\n",
        "\n",
        "        dist = self.cos(patient1, patient2).unsqueeze(dim=1)\n",
        "        dist = self.sm(dist)\n",
        "        print(dist[0])\n",
        "        return torch.abs(dist)"
      ],
      "metadata": {
        "id": "pNKI6PYgJY1h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#CNN_Softmax model definition\n",
        "class CNN_Softmax_custom(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv_a = torch.nn.Conv1d(in_channels=10, \n",
        "                                      out_channels=25, \n",
        "                                      kernel_size=1, bias=False)\n",
        "        self.conv_b = torch.nn.Conv1d(in_channels=25, \n",
        "                                      out_channels=37, \n",
        "                                      kernel_size=1)\n",
        "        self.conv_c = torch.nn.Conv1d(in_channels=37, \n",
        "                                      out_channels=60, \n",
        "                                      kernel_size=1)\n",
        "        self.maxpool_a = torch.nn.MaxPool1d(kernel_size=1,\n",
        "                                            stride=1)\n",
        "        self.maxpool_b = torch.nn.MaxPool1d(kernel_size=1,\n",
        "                                            stride=1)\n",
        "        self.maxpool_c = torch.nn.MaxPool1d(kernel_size=1,\n",
        "                                            stride=1)\n",
        "        self.fc = torch.nn.Linear(in_features=60, \n",
        "                                  out_features=10)\n",
        "        self.sm_fc = torch.nn.Linear(in_features=10010,\n",
        "                                     out_features=1)\n",
        "        self.sm_fc2 = torch.nn.Linear(in_features=10,\n",
        "                                      out_features=1)\n",
        "        self.sm = torch.nn.Softmax(dim=1)\n",
        "\n",
        "    def forward_single(self, x):\n",
        "        #x = torch.permute(x, (0, 2, 1))\n",
        "        x = self.conv_a(x)\n",
        "        x = F.relu(x, inplace=True)\n",
        "        x = self.maxpool_a(x)\n",
        "\n",
        "        #print('Finished first conv')\n",
        "        x = self.conv_b(x)\n",
        "        x = F.relu(x, inplace=True)\n",
        "        x = self.maxpool_b(x)\n",
        "\n",
        "        x = self.conv_c(x)\n",
        "        x = F.relu(x, inplace=True)\n",
        "        x = self.maxpool_c(x).squeeze()\n",
        "\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        x = x.unsqueeze(dim=2).float()\n",
        "        y = y.unsqueeze(dim=2).float()\n",
        "        patient1 = self.forward_single(x)\n",
        "        patient2 = self.forward_single(y)\n",
        "\n",
        "        matching_matrix = torch.matmul(self.fc.weight, self.fc.weight.T)\n",
        "\n",
        "        S = torch.matmul(patient1, patient2.T)\n",
        "        K = torch.add(patient1, patient2)\n",
        "\n",
        "        ks_cat = torch.cat((S, K), dim=1)\n",
        "        score=ks_cat\n",
        "        score = self.sm(score)\n",
        "        score = self.sm_fc(score)\n",
        "\n",
        "        return 1-torch.abs(score)\n"
      ],
      "metadata": {
        "id": "fb0uysJTg_zK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "an0st-izLds6"
      },
      "outputs": [],
      "source": [
        "#Train\n",
        "import torch.optim as optim\n",
        "\n",
        "print(device)\n",
        "#net = CNN_Softmax_old().to(device)\n",
        "#net = CNN_Eucledian().to(device)\n",
        "#net = CNN_Cosine().to(device)\n",
        "net = CNN_Softmax_custom().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net.parameters(), lr=0.01)\n",
        "\n",
        "\n",
        "loss_values = []\n",
        "vanishing = False\n",
        "for epoch in (range(20)):  # loop over the dataset multiple times\n",
        "    running_loss = []\n",
        "    for i, data in enumerate(train_loader):\n",
        "        # get the inputs\n",
        "        x, y, label = data\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        #print(\"loader shape: \" + str(label.shape))\n",
        "        #print(label[:10])\n",
        "        outputs = net(x, y)\n",
        "\n",
        "        label = label.unsqueeze(dim=1)\n",
        "        #outputs = outputs.unsqueeze(dim=1).squeeze(dim=2)\n",
        "\n",
        "        loss = criterion(outputs, label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        running_loss.append(loss.item())\n",
        "        #print(running_loss)\n",
        "        #if(loss.item() == np.mean(running_loss) and len(running_loss) != 0):\n",
        "          #print(\"Vanishing gradient: \" + str(loss.item()))\n",
        "          #vanishing=True\n",
        "          #break\n",
        "\n",
        "    loss_values.append(np.mean(running_loss))\n",
        "    print(np.mean(running_loss))\n",
        "    p, r, f, acc = eval(net, test_loader)\n",
        "    print(p)\n",
        "    print(r)\n",
        "    print(f)\n",
        "    print(acc)\n",
        "    #validate(test_loader, net, criterion)\n",
        "    torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': net.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': loss,\n",
        "            }, \"epoch_model.tar\")\n",
        "\n",
        "\n",
        "print('Finished Training')\n",
        "print(loss_values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qh38gE2Kc8L_"
      },
      "outputs": [],
      "source": [
        "#Saving model\n",
        "torch.save(net, \"full_model.pt\")\n",
        "torch.save(net.state_dict(), \"model_dict.pt\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "similarity_full",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}