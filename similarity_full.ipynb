{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4mPcC1aa83O"
      },
      "source": [
        "# CNN_Softmax - CS 598 DLH - Reproducable Paper Final Project\n",
        "\n",
        "This contains the code required for the paper titled \"A Novel Deep Similarity Learning Approach to Electronic Health Records Data\"\n",
        "\n",
        "Please note some of the code portions might have been used from MY OWN code of CS598-DLH lab/mps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PTR-kLB4FG0q"
      },
      "source": [
        "## Environment setup + Data pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v1fal44MBuSf",
        "outputId": "0ed82460-9f5a-4a48-f711-13335789f5dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.7/dist-packages (4.4.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from gdown) (4.64.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from gdown) (3.6.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.7/dist-packages (from gdown) (2.23.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from gdown) (4.6.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gdown) (1.15.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (1.24.3)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "mkdir: cannot create directory ‘data’: File exists\n",
            "NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Environment setup\n",
        "!pip3 install gdown\n",
        "!mkdir data\n",
        "!nvidia-smi\n",
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "\n",
        "#Imports\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import gdown\n",
        "import pandas as pd\n",
        "from tabulate import tabulate\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn as nn\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PlNpKdN1oHJ7",
        "outputId": "4de31c4d-4440-4d0a-9dd0-46a8c0c22168"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1uBv9j602LGyN43wvbQDpvka49rR9eNUQ\n",
            "To: /content/data/orbda.csv\n",
            "100%|██████████| 818M/818M [00:06<00:00, 130MB/s]\n"
          ]
        }
      ],
      "source": [
        "#https://drive.google.com/uc?id=1uBv9j602LGyN43wvbQDpvka49rR9eNUQ&export=download&confirm=t\n",
        "'''from google_drive_downloader import GoogleDriveDownloader as gdd\n",
        "gdd.download_file_from_google_drive(file_id='1uBv9j602LGyN43wvbQDpvka49rR9eNUQ',\n",
        "                                    dest_path='./data/orbda.csv',\n",
        "                                    unzip=True,\n",
        "                                    showsize=True,\n",
        "                                    overwrite=True)'''\n",
        "\n",
        "\n",
        "url = \"https://drive.google.com/uc?id=1uBv9j602LGyN43wvbQDpvka49rR9eNUQ\"\n",
        "output = \"./data/orbda.csv\"\n",
        "gdown.download(url, output, quiet=False)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "df = pd.read_csv('data/orbda.csv', low_memory=False) #read in csv file (~800mb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "f5hQPH09ET1m"
      },
      "outputs": [],
      "source": [
        "#Sanity checks for pandas import + read_csv\n",
        "#print(df.columns)\n",
        "#print(sorted(df['ap_cidpri'].unique()))\n",
        "#df.head()\n",
        "#print(df['id_'].nunique())\n",
        "#print(df['id_'].count())\n",
        "#print(df['ap_coduni'].unique())\n",
        "#print(df['ap_coduni'].count())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXqUq-5kAVw1"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "4T5yj_6O1XAe"
      },
      "outputs": [],
      "source": [
        "#Filtering out for specific values as outlined in Table 3\n",
        "kidney_codes = ['E10 ', 'E14 ', 'I10 ', 'I120', 'N039', 'N088', 'N083', 'N180', 'N188', 'N189']\n",
        "df = df.loc[df['ap_cidpri'].isin(kidney_codes)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oPvj4uyPKBNN",
        "outputId": "1aafa818-e291-426d-ba19-e79c1db3d038"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----------+----------+------------+--------------+------------------+-------------+-------------+----------+----------+-------------+-------------+\n",
            "|    | an_hcv   | an_hiv   | an_hbsag   |   ap_nuidade | ap_coduni        |   ap_pripal |   ap_motsai | estado   |   an_tru |   an_intfis | ap_cidpri   |\n",
            "|----+----------+----------+------------+--------------+------------------+-------------+-------------+----------+----------+-------------+-------------|\n",
            "|  0 | N        | N        | N          |           25 | a1042cb8e9265d4e |   305010107 |          21 | MG       |     628  |          03 | N180        |\n",
            "|  1 | N        | N        | N          |           47 | 69ba059ff91532d3 |   305010107 |          21 | RJ       |     0065 |          00 | N180        |\n",
            "|  2 | N        | N        | N          |           15 | a2b516fa1aa3cce0 |   305010107 |          21 | PR       |     0    |          01 | N180        |\n",
            "|  3 | N        | N        | N          |           37 | 72f15d07e504318f |   305010107 |          21 | RJ       |     6582 |          01 | N180        |\n",
            "|  4 | N        | N        | N          |           71 | 72f15d07e504318f |   305010107 |          21 | RJ       |     7778 |          01 | N180        |\n",
            "+----+----------+----------+------------+--------------+------------------+-------------+-------------+----------+----------+-------------+-------------+\n"
          ]
        }
      ],
      "source": [
        "#Filtering out columns to match input features\n",
        "#Features: an_hcv, an_hiv, an_hbsag, ap_nuidade, ap_coduni, owner_id, ap_pripal, ap_motsai, estado, an_tru, an_intfis, vol\n",
        "column_features = ['an_hcv', 'an_hiv', 'an_hbsag', 'ap_nuidade', 'ap_coduni', 'ap_pripal', 'ap_motsai', 'estado', 'an_tru', 'an_intfis', 'ap_cidpri']\n",
        "df_input_features = df.filter(items=column_features)\n",
        "print(tabulate(df_input_features.head(), headers = 'keys', tablefmt = 'psql'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e3grtwvAA4CG",
        "outputId": "85fde27d-beca-4de3-f72f-6b879395fa45"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----------+----------+------------+--------------+-------------+-------------+-------------+----------+----------+-------------+-------------+\n",
            "|    | an_hcv   | an_hiv   | an_hbsag   |   ap_nuidade |   ap_coduni |   ap_pripal |   ap_motsai | estado   |   an_tru |   an_intfis | ap_cidpri   |\n",
            "|----+----------+----------+------------+--------------+-------------+-------------+-------------+----------+----------+-------------+-------------|\n",
            "|  0 | N        | N        | N          |           25 |           0 |   305010107 |          21 | MG       |     628  |          03 | N180        |\n",
            "|  1 | N        | N        | N          |           47 |           1 |   305010107 |          21 | RJ       |     0065 |          00 | N180        |\n",
            "|  2 | N        | N        | N          |           15 |           2 |   305010107 |          21 | PR       |     0    |          01 | N180        |\n",
            "|  3 | N        | N        | N          |           37 |           3 |   305010107 |          21 | RJ       |     6582 |          01 | N180        |\n",
            "|  4 | N        | N        | N          |           71 |           3 |   305010107 |          21 | RJ       |     7778 |          01 | N180        |\n",
            "+----+----------+----------+------------+--------------+-------------+-------------+-------------+----------+----------+-------------+-------------+\n"
          ]
        }
      ],
      "source": [
        "#Converting ap_coduni values into int via dict\n",
        "coduni_dict = {}\n",
        "unique_coduni = df_input_features['ap_coduni'].unique()\n",
        "for i in range(0, len(unique_coduni)):\n",
        "  coduni_dict[unique_coduni[i]] = i\n",
        "df_input_features = df_input_features.replace({'ap_coduni': coduni_dict})\n",
        "print(tabulate(df_input_features.head(), headers = 'keys', tablefmt = 'psql'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WnlKRngRCaka",
        "outputId": "30b36a99-f795-411e-c4cb-cd3e2880fff5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----------+----------+------------+--------------+-------------+-------------+-------------+----------+----------+-------------+-------------+\n",
            "|    |   an_hcv |   an_hiv |   an_hbsag |   ap_nuidade |   ap_coduni |   ap_pripal |   ap_motsai | estado   |   an_tru |   an_intfis | ap_cidpri   |\n",
            "|----+----------+----------+------------+--------------+-------------+-------------+-------------+----------+----------+-------------+-------------|\n",
            "|  0 |        0 |        0 |          0 |           25 |           0 |   305010107 |          21 | MG       |     628  |          03 | N180        |\n",
            "|  1 |        0 |        0 |          0 |           47 |           1 |   305010107 |          21 | RJ       |     0065 |          00 | N180        |\n",
            "|  2 |        0 |        0 |          0 |           15 |           2 |   305010107 |          21 | PR       |     0    |          01 | N180        |\n",
            "|  3 |        0 |        0 |          0 |           37 |           3 |   305010107 |          21 | RJ       |     6582 |          01 | N180        |\n",
            "|  4 |        0 |        0 |          0 |           71 |           3 |   305010107 |          21 | RJ       |     7778 |          01 | N180        |\n",
            "+----+----------+----------+------------+--------------+-------------+-------------+-------------+----------+----------+-------------+-------------+\n"
          ]
        }
      ],
      "source": [
        "#Converting an_hcv, an_hiv, an_hbsag to int values from dict\n",
        "yes_no_dict = {'N': 0, \"P\": 1}\n",
        "df_input_features = df_input_features.replace({'an_hcv': yes_no_dict, 'an_hiv': yes_no_dict, 'an_hbsag': yes_no_dict})\n",
        "print(tabulate(df_input_features.head(), headers = 'keys', tablefmt = 'psql'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1zZ0wVXJG71",
        "outputId": "260624b2-b35b-4b6b-ade4-a79407dc77c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----------+----------+------------+--------------+-------------+-------------+-------------+----------+----------+-------------+-------------+\n",
            "|    |   an_hcv |   an_hiv |   an_hbsag |   ap_nuidade |   ap_coduni |   ap_pripal |   ap_motsai |   estado |   an_tru |   an_intfis | ap_cidpri   |\n",
            "|----+----------+----------+------------+--------------+-------------+-------------+-------------+----------+----------+-------------+-------------|\n",
            "|  0 |        0 |        0 |          0 |           25 |           0 |   305010107 |          21 |        0 |     628  |          03 | N180        |\n",
            "|  1 |        0 |        0 |          0 |           47 |           1 |   305010107 |          21 |        1 |     0065 |          00 | N180        |\n",
            "|  2 |        0 |        0 |          0 |           15 |           2 |   305010107 |          21 |        2 |     0    |          01 | N180        |\n",
            "|  3 |        0 |        0 |          0 |           37 |           3 |   305010107 |          21 |        1 |     6582 |          01 | N180        |\n",
            "|  4 |        0 |        0 |          0 |           71 |           3 |   305010107 |          21 |        1 |     7778 |          01 | N180        |\n",
            "+----+----------+----------+------------+--------------+-------------+-------------+-------------+----------+----------+-------------+-------------+\n"
          ]
        }
      ],
      "source": [
        "#Converting estado to int values from dict\n",
        "estado_dict = {}\n",
        "unique_estado = df_input_features['estado'].unique()\n",
        "for i in range(0, len(unique_estado)):\n",
        "  estado_dict[unique_estado[i]] = i\n",
        "df_input_features = df_input_features.replace({'estado': estado_dict})\n",
        "print(tabulate(df_input_features.head(), headers = 'keys', tablefmt = 'psql'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "CBLWyefuPMhE"
      },
      "outputs": [],
      "source": [
        "#Converting an_tru to int values from dict \n",
        "df_input_features['an_tru'] = df_input_features['an_tru'].str.strip()\n",
        "df_input_features['an_tru'] = df_input_features['an_tru'].str.extract('(\\d+)', expand=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oOKgAGypMdL9",
        "outputId": "1809ed8a-8e25-4693-8252-c52a55de9ecb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----------+----------+------------+--------------+-------------+-------------+-------------+----------+----------+-------------+-------------+\n",
            "|    |   an_hcv |   an_hiv |   an_hbsag |   ap_nuidade |   ap_coduni |   ap_pripal |   ap_motsai |   estado |   an_tru |   an_intfis | ap_cidpri   |\n",
            "|----+----------+----------+------------+--------------+-------------+-------------+-------------+----------+----------+-------------+-------------|\n",
            "|  0 |        0 |        0 |          0 |           25 |           0 |   305010107 |          21 |        0 |      628 |           0 | N180        |\n",
            "|  1 |        0 |        0 |          0 |           47 |           1 |   305010107 |          21 |        1 |     0065 |           1 | N180        |\n",
            "|  2 |        0 |        0 |          0 |           15 |           2 |   305010107 |          21 |        2 |        0 |           2 | N180        |\n",
            "|  3 |        0 |        0 |          0 |           37 |           3 |   305010107 |          21 |        1 |     6582 |           2 | N180        |\n",
            "|  4 |        0 |        0 |          0 |           71 |           3 |   305010107 |          21 |        1 |     7778 |           2 | N180        |\n",
            "+----+----------+----------+------------+--------------+-------------+-------------+-------------+----------+----------+-------------+-------------+\n"
          ]
        }
      ],
      "source": [
        "#Converting an_intfis to int values from dict \n",
        "\n",
        "intfis_dict = {}\n",
        "unique_intfis = df_input_features['an_intfis'].unique()\n",
        "for i in range(0, len(unique_intfis)):\n",
        "  intfis_dict[unique_intfis[i]] = i\n",
        "df_input_features = df_input_features.replace({'an_intfis': intfis_dict})\n",
        "print(tabulate(df_input_features.head(), headers = 'keys', tablefmt = 'psql'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ZStaoWUyLjB4"
      },
      "outputs": [],
      "source": [
        "#Converting pandas object types into integer\n",
        "df_input_features = df_input_features.dropna()\n",
        "df_input_features['an_hcv'] = df_input_features['an_hcv'].astype('str').astype('float')\n",
        "df_input_features['an_hiv'] = df_input_features['an_hiv'].astype('str').astype('float')\n",
        "df_input_features['an_hbsag'] = df_input_features['an_hbsag'].astype('str').astype('float')\n",
        "df_input_features['an_tru'] = df_input_features['an_tru'].astype('str').astype('float')\n",
        "df_input_features['an_intfis'] = df_input_features['an_intfis'].astype('str').astype('float')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h89LWhmqB5sk",
        "outputId": "23db6d33-9196-4f6b-850c-1a59d412027c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----------------+\n",
            "|      |       ap_cidpri |\n",
            "|------+-----------------|\n",
            "| N180 |     3.94376e+06 |\n",
            "| N189 | 38160           |\n",
            "| I120 | 19460           |\n",
            "| N039 |  7349           |\n",
            "| I10  |  5979           |\n",
            "| N083 |  5782           |\n",
            "| N188 |  4670           |\n",
            "| N088 |  1698           |\n",
            "| E10  |   190           |\n",
            "| E14  |   168           |\n",
            "+------+-----------------+\n",
            "+----+----------+----------+------------+--------------+-------------+-------------+-------------+----------+----------+-------------+-------------+\n",
            "|    |   an_hcv |   an_hiv |   an_hbsag |   ap_nuidade |   ap_coduni |   ap_pripal |   ap_motsai |   estado |   an_tru |   an_intfis | ap_cidpri   |\n",
            "|----+----------+----------+------------+--------------+-------------+-------------+-------------+----------+----------+-------------+-------------|\n",
            "|  0 |        0 |        0 |          0 |           25 |           0 |   305010107 |          21 |        0 |      628 |           0 | N180        |\n",
            "|  1 |        0 |        0 |          0 |           47 |           1 |   305010107 |          21 |        1 |       65 |           1 | N180        |\n",
            "|  2 |        0 |        0 |          0 |           15 |           2 |   305010107 |          21 |        2 |        0 |           2 | N180        |\n",
            "|  3 |        0 |        0 |          0 |           37 |           3 |   305010107 |          21 |        1 |     6582 |           2 | N180        |\n",
            "|  4 |        0 |        0 |          0 |           71 |           3 |   305010107 |          21 |        1 |     7778 |           2 | N180        |\n",
            "+----+----------+----------+------------+--------------+-------------+-------------+-------------+----------+----------+-------------+-------------+\n",
            "an_hcv        float64\n",
            "an_hiv        float64\n",
            "an_hbsag      float64\n",
            "ap_nuidade      int64\n",
            "ap_coduni       int64\n",
            "ap_pripal       int64\n",
            "ap_motsai       int64\n",
            "estado          int64\n",
            "an_tru        float64\n",
            "an_intfis     float64\n",
            "ap_cidpri      object\n",
            "dtype: object\n"
          ]
        }
      ],
      "source": [
        "#Sanity checks post filtering\n",
        "#display(df.groupby('ap_cidpri')['ap_cidpri'].transform('count'))\n",
        "print(tabulate(df['ap_cidpri'].value_counts().to_frame(), headers = 'keys', tablefmt = 'psql'))\n",
        "print(tabulate(df_input_features.head(), headers = 'keys', tablefmt = 'psql'))\n",
        "print(df_input_features.dtypes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Vvesj7AoFPhC"
      },
      "outputs": [],
      "source": [
        "#Converting categorical lables to one hot encoding\n",
        "kidney_codes_dict = {'E10 ': 1, 'E14 ': 2, 'I10 ': 3, 'I120': 4, 'N039': 4, 'N088': 5, 'N083': 6, 'N180': 7, 'N188': 8, 'N189': 9}\n",
        "\n",
        "cidpri_list = df_input_features['ap_cidpri'].tolist()\n",
        "for i in range(0, len(cidpri_list)):\n",
        "  cidpri_list[i] = kidney_codes_dict[cidpri_list[i]]\n",
        "\n",
        "tensor_cidpri = torch.tensor(cidpri_list)\n",
        "cidpri_one_hot = F.one_hot(tensor_cidpri)\n",
        "\n",
        "#Drop cidpri column since we are now done with it\n",
        "df_input_features = df_input_features.drop(['ap_cidpri'], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "2vRPoBS8qlzw"
      },
      "outputs": [],
      "source": [
        "#Dataloader calss\n",
        "import random\n",
        "class NephrologyDataset(Dataset):\n",
        "  def __init__(self, input_features, categorical_features):\n",
        "    x = input_features.values\n",
        "    y = categorical_features\n",
        "    #print(y)\n",
        "\n",
        "    self.x_train = x#torch.tensor(x, device=device)\n",
        "    self.y_train = cidpri_list#y#y.to(device)#torch.tensor(y, device=device)\n",
        "\n",
        "  def __len__(self):\n",
        "    return math.floor(len(self.y_train)/2)\n",
        "  \n",
        "  def __getitem__(self, idx):\n",
        "    if (idx + 2> self.__len__()):\n",
        "      idx = idx-3\n",
        "    second_idx = random.randint(0, self.__len__()-1)\n",
        "    return torch.tensor(self.x_train[idx], device=device), torch.tensor(self.x_train[second_idx], device=device), torch.tensor((self.y_train[second_idx] == self.y_train[idx]), device=device).float()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7YTJRDfGsZIN",
        "outputId": "a957c8bf-d034-4c17-89fe-db961737f699"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3\n",
            "tensor([1.0000e+00, 0.0000e+00, 1.0000e+00, 5.9000e+01, 6.1900e+02, 3.0501e+08,\n",
            "        2.1000e+01, 4.0000e+00, 7.7000e+01, 5.0000e+00], dtype=torch.float64)\n",
            "tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 4.4000e+01, 1.4700e+02, 3.0501e+08,\n",
            "        2.1000e+01, 8.0000e+00, 9.6000e+01, 9.0000e+00], dtype=torch.float64)\n",
            "tensor(1.)\n"
          ]
        }
      ],
      "source": [
        "#Dataloader initialization\n",
        "split_train = int((len(cidpri_one_hot)) * .8)\n",
        "split_test = int((len(cidpri_one_hot)) * .2)\n",
        "dataloader_train = NephrologyDataset(df_input_features[:split_train], cidpri_list[:split_train])\n",
        "dataloader_test = NephrologyDataset(df_input_features[:-split_test], cidpri_list[:-split_test])\n",
        "train_loader = DataLoader(dataloader_train,batch_size=10000,shuffle=True, drop_last=True)\n",
        "test_loader = DataLoader(dataloader_test,batch_size=10000,shuffle=True, drop_last=True)\n",
        "\n",
        "sample_ex = next(iter(test_loader))\n",
        "print(len(sample_ex))\n",
        "x, y, l = sample_ex\n",
        "print(x[0])\n",
        "print(y[0])\n",
        "print(l[0])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#CNN_Softmax model definition\n",
        "class CNN_Eucledian(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv_a = torch.nn.Conv1d(in_channels=10, \n",
        "                                      out_channels=25, \n",
        "                                      kernel_size=1, bias=False)\n",
        "        self.conv_b = torch.nn.Conv1d(in_channels=25, \n",
        "                                      out_channels=37, \n",
        "                                      kernel_size=1)\n",
        "        self.maxpool_a = torch.nn.MaxPool1d(kernel_size=1,\n",
        "                                            stride=1)\n",
        "        self.maxpool_b = torch.nn.MaxPool1d(kernel_size=1,\n",
        "                                            stride=1)\n",
        "        self.fc = torch.nn.Linear(in_features=37, \n",
        "                                  out_features=10)\n",
        "        self.sm_fc = torch.nn.Linear(in_features=10000,\n",
        "                                     out_features=1)\n",
        "        self.sm_fc2 = torch.nn.Linear(in_features=10,\n",
        "                                      out_features=1)\n",
        "        self.sm = torch.nn.Softmax(dim=1)\n",
        "\n",
        "    def forward_single(self, x):\n",
        "        #x = torch.permute(x, (0, 2, 1))\n",
        "        x = self.conv_a(x)\n",
        "        x = F.relu(x, inplace=True)\n",
        "        x = self.maxpool_a(x)\n",
        "\n",
        "        #print('Finished first conv')\n",
        "        x = self.conv_b(x)\n",
        "        x = F.relu(x, inplace=True)\n",
        "        x = self.maxpool_b(x).squeeze()\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        #for name, param in self.named_parameters():\n",
        "        x = x.unsqueeze(dim=2).float()\n",
        "        y = y.unsqueeze(dim=2).float()\n",
        "        patient1 = self.forward_single(x)\n",
        "        patient2 = self.forward_single(y)\n",
        "\n",
        "        dist = torch.cdist(patient1, patient2)\n",
        "        dist = self.sm(dist)\n",
        "        dist = self.sm_fc(dist)\n",
        "        print(dist.shape)\n",
        "        return 1-torch.abs(dist)\n"
      ],
      "metadata": {
        "id": "1VtNTkSk0lRI"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 621
        },
        "id": "nFtW3v_KVuuR",
        "outputId": "a18ec0fa-acbb-4f1c-eb93-2f251ba65800"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  1%|          | 1/159 [00:00<01:21,  1.95it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1.], grad_fn=<SelectBackward0>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|▏         | 2/159 [00:01<01:21,  1.94it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1.], grad_fn=<SelectBackward0>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  2%|▏         | 3/159 [00:01<01:18,  1.98it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1.], grad_fn=<SelectBackward0>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  3%|▎         | 4/159 [00:01<01:08,  2.25it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1.], grad_fn=<SelectBackward0>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  3%|▎         | 5/159 [00:02<01:11,  2.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1.], grad_fn=<SelectBackward0>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  4%|▍         | 6/159 [00:02<01:05,  2.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1.], grad_fn=<SelectBackward0>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  4%|▍         | 7/159 [00:03<01:07,  2.25it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1.], grad_fn=<SelectBackward0>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  5%|▌         | 8/159 [00:03<01:01,  2.45it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1.], grad_fn=<SelectBackward0>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  6%|▌         | 9/159 [00:04<01:07,  2.22it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1.], grad_fn=<SelectBackward0>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  6%|▋         | 10/159 [00:04<01:01,  2.42it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1.], grad_fn=<SelectBackward0>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  7%|▋         | 11/159 [00:04<01:04,  2.30it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1.], grad_fn=<SelectBackward0>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  8%|▊         | 12/159 [00:05<00:59,  2.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1.], grad_fn=<SelectBackward0>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  8%|▊         | 13/159 [00:05<01:02,  2.33it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1.], grad_fn=<SelectBackward0>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  8%|▊         | 13/159 [00:05<01:07,  2.17it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-67-67607990902a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-67-67607990902a>\u001b[0m in \u001b[0;36meval\u001b[0;34m(model, val_loader)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1194\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1195\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1196\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1197\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    528\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-b62997f0dc15>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     17\u001b[0m       \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0msecond_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msecond_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msecond_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "#Test/Validate model\n",
        "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def validate(loader, model, criterion):                       \n",
        "    correct = 0                                               \n",
        "    total = 0                                                 \n",
        "    running_loss = []\n",
        "    model.eval()                                              \n",
        "    with torch.no_grad():                                     \n",
        "        for i, data in enumerate((loader)):\n",
        "            x, y, label = data      \n",
        "                                                              \n",
        "            outputs = net(x, y)\n",
        "            outputs = outputs > 0.5\n",
        "            outputs = outputs.float()\n",
        "            loss = criterion(outputs, label.unsqueeze(dim=1)) \n",
        "            #print(outputs.shape)\n",
        "            outputs = torch.round(outputs)\n",
        "            total += label.shape[0]\n",
        "            correct += np.sum(outputs.squeeze().tolist() == label.squeeze().tolist())\n",
        "            running_loss.append(loss.item())\n",
        "            print(correct)      \n",
        "    mean_val_accuracy = (100 * correct / total)               \n",
        "    mean_val_loss = np.mean(running_loss)                  \n",
        "    mean_val_accuracy = accuracy(outputs,labels)       \n",
        "    print('Validation Accuracy: %d %%' % (mean_val_accuracy)) \n",
        "    print('Validation Loss:'  ,mean_val_loss )  \n",
        "\n",
        "def eval(model, val_loader):\n",
        "    \n",
        "    \"\"\"\n",
        "    Evaluate the model.\n",
        "    \n",
        "    Arguments:\n",
        "        model: the RNN model\n",
        "        val_loader: validation dataloader\n",
        "        \n",
        "    Outputs:\n",
        "        precision: overall precision score\n",
        "        recall: overall recall score\n",
        "        f1: overall f1 score\n",
        "        roc_auc: overall roc_auc score\n",
        "        \n",
        "    REFERENCE: checkout https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics\n",
        "    \"\"\"\n",
        "    \n",
        "    model.eval()\n",
        "    y_pred = torch.LongTensor()\n",
        "    y_score = torch.Tensor()\n",
        "    y_true = torch.LongTensor()\n",
        "    model.eval()\n",
        "    for i, data in enumerate(tqdm(val_loader)):\n",
        "        x, y, label = data      \n",
        "        outputs = net(x, y)\n",
        "\n",
        "        #https://discuss.pytorch.org/t/confused-about-binary-classification-with-pytorch/83759/10\n",
        "        y_hat = (outputs >= 0.8).float()\n",
        "        #print(y_hat[:10])\n",
        "        \n",
        "        y_pred = torch.cat((y_pred,  y_hat.detach().to('cpu')), dim=0)\n",
        "        y_true = torch.cat((y_true, label.detach().to('cpu')), dim=0)\n",
        "    \n",
        "    p, r, f, _ = precision_recall_fscore_support(y_true, y_pred, average='binary')\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    return p, r, f, acc\n",
        "\n",
        "p, r, f, acc = eval(net, test_loader)\n",
        "print(p)\n",
        "print(r)\n",
        "print(f)\n",
        "print(acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "tsRVyI_vAmzS"
      },
      "outputs": [],
      "source": [
        "#CNN_Softmax model definition\n",
        "class CNN_Softmax_old(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv_a = torch.nn.Conv1d(in_channels=10, \n",
        "                                      out_channels=25, \n",
        "                                      kernel_size=1, bias=False)\n",
        "        self.conv_b = torch.nn.Conv1d(in_channels=25, \n",
        "                                      out_channels=37, \n",
        "                                      kernel_size=1)\n",
        "        self.maxpool_a = torch.nn.MaxPool1d(kernel_size=1,\n",
        "                                            stride=1)\n",
        "        self.maxpool_b = torch.nn.MaxPool1d(kernel_size=1,\n",
        "                                            stride=1)\n",
        "        self.fc = torch.nn.Linear(in_features=37, \n",
        "                                  out_features=10)\n",
        "        self.sm_fc = torch.nn.Linear(in_features=10010,\n",
        "                                     out_features=1)\n",
        "        self.sm_fc2 = torch.nn.Linear(in_features=10,\n",
        "                                      out_features=1)\n",
        "        self.sm = torch.nn.Softmax(dim=1)\n",
        "\n",
        "    def forward_single(self, x):\n",
        "        #x = torch.permute(x, (0, 2, 1))\n",
        "        x = self.conv_a(x)\n",
        "        x = F.relu(x, inplace=True)\n",
        "        x = self.maxpool_a(x)\n",
        "\n",
        "        #print('Finished first conv')\n",
        "        x = self.conv_b(x)\n",
        "        x = F.relu(x, inplace=True)\n",
        "        x = self.maxpool_b(x).squeeze()\n",
        "        x = self.fc(x)\n",
        "        #x = F.relu(x)\n",
        "        #x = self.sm(x)\n",
        "        #print(x.shape)\n",
        "        #x = self.sm_fc2(x)\n",
        "        #print(x.shape)\n",
        "        return x\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        #for name, param in self.named_parameters():\n",
        "          #print(name, param.shape)\n",
        "        x = x.unsqueeze(dim=2).float()\n",
        "        y = y.unsqueeze(dim=2).float()\n",
        "        patient1 = self.forward_single(x)\n",
        "        patient2 = self.forward_single(y)\n",
        "\n",
        "        matching_matrix = torch.matmul(self.fc.weight, self.fc.weight.T)\n",
        "\n",
        "        S = torch.matmul(patient1, patient2.T)\n",
        "        K = torch.add(patient1, patient2)\n",
        "\n",
        "        #print(\"S: \" + str(S.shape))\n",
        "        #print(\"K: \" + str(K.shape))\n",
        "        ks_cat = torch.cat((S, K), dim=1)\n",
        "        #print(\"kscat: \" + str(ks_cat.shape))\n",
        "        #print(score.shape)\n",
        "        score=ks_cat\n",
        "        score = self.sm(score)\n",
        "        #score = F.relu(x).squeeze()\n",
        "        #print(score.shape)\n",
        "        score = self.sm_fc(score)\n",
        "        #score = torch.round(score)\n",
        "        #score = (torch.max(K, dim=1)[0]).float()\n",
        "        #score = score.clone().detach().requires_grad_(True)\n",
        "        #print(\"score: \" + str(score.shape))\n",
        "        #print(score[:11])\n",
        "        return 1-torch.abs(score)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#CNN_Softmax model definition\n",
        "class CNN_Cosine(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv_a = torch.nn.Conv1d(in_channels=10, \n",
        "                                      out_channels=25, \n",
        "                                      kernel_size=1, bias=False)\n",
        "        self.conv_b = torch.nn.Conv1d(in_channels=25, \n",
        "                                      out_channels=37, \n",
        "                                      kernel_size=1)\n",
        "        self.maxpool_a = torch.nn.MaxPool1d(kernel_size=1,\n",
        "                                            stride=1)\n",
        "        self.maxpool_b = torch.nn.MaxPool1d(kernel_size=1,\n",
        "                                            stride=1)\n",
        "        self.fc = torch.nn.Linear(in_features=37, \n",
        "                                  out_features=10)\n",
        "        self.sm_fc = torch.nn.Linear(in_features=10000,\n",
        "                                     out_features=1)\n",
        "        self.sm_fc2 = torch.nn.Linear(in_features=10,\n",
        "                                      out_features=1)\n",
        "        self.cos = torch.nn.CosineSimilarity(dim=1)\n",
        "        self.sm = torch.nn.Softmax(dim=1)\n",
        "\n",
        "    def forward_single(self, x):\n",
        "        #x = torch.permute(x, (0, 2, 1))\n",
        "        x = self.conv_a(x)\n",
        "        x = F.relu(x, inplace=True)\n",
        "        x = self.maxpool_a(x)\n",
        "\n",
        "        #print('Finished first conv')\n",
        "        x = self.conv_b(x)\n",
        "        x = F.relu(x, inplace=True)\n",
        "        x = self.maxpool_b(x).squeeze()\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        #for name, param in self.named_parameters():\n",
        "        x = x.unsqueeze(dim=2).float()\n",
        "        y = y.unsqueeze(dim=2).float()\n",
        "        patient1 = self.forward_single(x)\n",
        "        patient2 = self.forward_single(y)\n",
        "\n",
        "        dist = self.cos(patient1, patient2).unsqueeze(dim=1)\n",
        "        dist = self.sm(dist)\n",
        "        print(dist[0])\n",
        "        return torch.abs(dist)"
      ],
      "metadata": {
        "id": "pNKI6PYgJY1h"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#CNN_Softmax model definition\n",
        "class CNN_Softmax_custom(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv_a = torch.nn.Conv1d(in_channels=10, \n",
        "                                      out_channels=25, \n",
        "                                      kernel_size=1, bias=False)\n",
        "        self.conv_b = torch.nn.Conv1d(in_channels=25, \n",
        "                                      out_channels=37, \n",
        "                                      kernel_size=1)\n",
        "        self.conv_c = torch.nn.Conv1d(in_channels=37, \n",
        "                                      out_channels=60, \n",
        "                                      kernel_size=1)\n",
        "        self.maxpool_a = torch.nn.MaxPool1d(kernel_size=1,\n",
        "                                            stride=1)\n",
        "        self.maxpool_b = torch.nn.MaxPool1d(kernel_size=1,\n",
        "                                            stride=1)\n",
        "        self.maxpool_c = torch.nn.MaxPool1d(kernel_size=1,\n",
        "                                            stride=1)\n",
        "        self.fc = torch.nn.Linear(in_features=60, \n",
        "                                  out_features=10)\n",
        "        self.sm_fc = torch.nn.Linear(in_features=10010,\n",
        "                                     out_features=1)\n",
        "        self.sm_fc2 = torch.nn.Linear(in_features=10,\n",
        "                                      out_features=1)\n",
        "        self.sm = torch.nn.Softmax(dim=1)\n",
        "\n",
        "    def forward_single(self, x):\n",
        "        #x = torch.permute(x, (0, 2, 1))\n",
        "        x = self.conv_a(x)\n",
        "        x = F.relu(x, inplace=True)\n",
        "        x = self.maxpool_a(x)\n",
        "\n",
        "        #print('Finished first conv')\n",
        "        x = self.conv_b(x)\n",
        "        x = F.relu(x, inplace=True)\n",
        "        x = self.maxpool_b(x)\n",
        "\n",
        "        x = self.conv_c(x)\n",
        "        x = F.relu(x, inplace=True)\n",
        "        x = self.maxpool_c(x).squeeze()\n",
        "\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        x = x.unsqueeze(dim=2).float()\n",
        "        y = y.unsqueeze(dim=2).float()\n",
        "        patient1 = self.forward_single(x)\n",
        "        patient2 = self.forward_single(y)\n",
        "\n",
        "        matching_matrix = torch.matmul(self.fc.weight, self.fc.weight.T)\n",
        "\n",
        "        S = torch.matmul(patient1, patient2.T)\n",
        "        K = torch.add(patient1, patient2)\n",
        "\n",
        "        ks_cat = torch.cat((S, K), dim=1)\n",
        "        score=ks_cat\n",
        "        score = self.sm(score)\n",
        "        score = self.sm_fc(score)\n",
        "\n",
        "        return 1-torch.abs(score)\n"
      ],
      "metadata": {
        "id": "fb0uysJTg_zK"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "an0st-izLds6",
        "outputId": "66a96627-4a1f-466d-8263-d0747685b2cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n",
            "0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 159/159 [02:18<00:00,  1.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9633943396226415\n",
            "1.0\n",
            "0.9813559305746017\n",
            "0.9633943396226415\n",
            "Finished Training\n",
            "[0.0]\n"
          ]
        }
      ],
      "source": [
        "#Train\n",
        "import torch.optim as optim\n",
        "\n",
        "print(device)\n",
        "#net = CNN_Softmax_old().to(device)\n",
        "#net = CNN_Eucledian().to(device)\n",
        "#net = CNN_Cosine().to(device)\n",
        "net = CNN_Softmax_custom().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net.parameters(), lr=0.01)\n",
        "\n",
        "\n",
        "loss_values = []\n",
        "vanishing = False\n",
        "for epoch in (range(1)):  # loop over the dataset multiple times\n",
        "    running_loss = []\n",
        "    for i, data in enumerate(train_loader):\n",
        "        # get the inputs\n",
        "        x, y, label = data\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        #print(\"loader shape: \" + str(label.shape))\n",
        "        #print(label[:10])\n",
        "        outputs = net(x, y)\n",
        "\n",
        "        label = label.unsqueeze(dim=1)\n",
        "        #outputs = outputs.unsqueeze(dim=1).squeeze(dim=2)\n",
        "\n",
        "        loss = criterion(outputs, label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        running_loss.append(loss.item())\n",
        "        #print(running_loss)\n",
        "        #if(loss.item() == np.mean(running_loss) and len(running_loss) != 0):\n",
        "          #print(\"Vanishing gradient: \" + str(loss.item()))\n",
        "          #vanishing=True\n",
        "          #break\n",
        "\n",
        "    loss_values.append(np.mean(running_loss))\n",
        "    print(np.mean(running_loss))\n",
        "    p, r, f, acc = eval(net, test_loader)\n",
        "    print(p)\n",
        "    print(r)\n",
        "    print(f)\n",
        "    print(acc)\n",
        "    #validate(test_loader, net, criterion)\n",
        "    torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': net.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': loss,\n",
        "            }, \"epoch_model.tar\")\n",
        "\n",
        "\n",
        "print('Finished Training')\n",
        "print(loss_values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "fri1ATDojgdL",
        "outputId": "12cc0539-649f-465e-aa9e-938f743fe61b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f8f88e21550>]"
            ]
          },
          "metadata": {},
          "execution_count": 62
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "#Graphs + Analysis\n",
        "plt.plot(np.squeeze(loss_values[1:]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "Qh38gE2Kc8L_"
      },
      "outputs": [],
      "source": [
        "#Saving model\n",
        "torch.save(net, \"full_model.pt\")\n",
        "torch.save(net.state_dict(), \"model_dict.pt\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "similarity_full",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}